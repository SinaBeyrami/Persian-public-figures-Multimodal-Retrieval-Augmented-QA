{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHQRwfO-kbc8"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!which pip3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install transformers pillow accelerate torch requests --upgrade\n",
    "!{sys.executable} -m pip install --upgrade \"transformers>=4.45.0\" torch pillow accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TDySaFdka7y"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union, Callable\n",
    "# from tqdm.auto import tqdm\n",
    "# from pathlib import Path\n",
    "from PIL import Image\n",
    "import os, re, json, unicodedata\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o9L3GIHcGwB"
   },
   "source": [
    "# Drive and Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "-f44gANXb-Qu",
    "outputId": "e3b89e27-e7bd-438b-ab73-18bcd9e85689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "from google.colab import userdata\n",
    "userdata.get(\"Hugging_Face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbVWHYcAb_HP",
    "outputId": "f18df377-8d6e-45a3-cf09-daeb98e790c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1IIN7uRgZgHRfMEwaoGwpD9ZCqjsSsXDL/project/Questions\n",
      "mcq_questions_90.json   multi_with_image_50.json       uni_questions_110.json\n",
      "mcq_with_image_50.json  multi_with_image_50_newV.json\n"
     ]
    }
   ],
   "source": [
    "%cd ./Questions/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCjVjwE0cNr3"
   },
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WG7XAP3IcLqC"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqS2Bpr_dd7R"
   },
   "source": [
    "# File names, paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNpPK3ZbdgLq"
   },
   "outputs": [],
   "source": [
    "MATCH_MODE = \"exact\"\n",
    "SAVE_DIR = \"/home/lona/Desktop/NLP_Final_Project/test_results/\"\n",
    "mcq_filename = \"mcq_questions_90.json\"\n",
    "mcq_multimodal_filename = \"mcq_with_image_50_final.json\"\n",
    "_110_unimodal_questions_without_answers_filename = \"uni_questions_110.json\"\n",
    "_50_multimodal_questions_without_answers_filename = \"multi_with_image_50_newV.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftnubFKgcXIr"
   },
   "source": [
    "# Test Phase Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8JkacK-cS8L"
   },
   "outputs": [],
   "source": [
    "# ---------- Normalization for evaluation ----------\n",
    "ARABIC_TO_PERSIAN = {\"\\u064a\":\"\\u06cc\", \"\\u0643\":\"\\u06a9\", \"\\u0629\":\"\\u0647\", \"\\u0649\":\"\\u06cc\", \"\\u0623\":\"\\u0627\", \"\\u0625\":\"\\u0627\"}\n",
    "FA2EN = str.maketrans(\"۰۱۲۳۴۵۶۷۸۹\", \"0123456789\")\n",
    "AR2EN = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
    "PUNCT = re.compile(r\"[^\\w\\s\\u0600-\\u06FF]\") # Delete some non alphanumeric punctuations\n",
    "\n",
    "# ---------- Utility Functions ----------\n",
    "def normalize_eval_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    for a,p in ARABIC_TO_PERSIAN.items(): s = s.replace(a,p)\n",
    "    s = s.translate(FA2EN).translate(AR2EN)\n",
    "    s = PUNCT.sub(\" \", s)\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "# def to_list_answers(ans: Union[str, List[str]]) -> List[str]:\n",
    "#     if ans is None: return []\n",
    "#     if isinstance(ans, list): return ans\n",
    "#     parts = re.split(r\"\\s*\\|\\s*|\\s*;\\s*|\\s*،،\\s*|\\s*،\\s*\", str(ans))\n",
    "#     parts = [p for p in parts if p]\n",
    "#     return parts if parts else [str(ans)]\n",
    "\n",
    "\n",
    "def exact_match(pred: str, golds: List[str]) -> bool:\n",
    "    p = normalize_eval_text(pred)\n",
    "    gs = [normalize_eval_text(g) for g in golds]\n",
    "    return any(p == g for g in gs)\n",
    "\n",
    "def relaxed_match(pred: str, golds: List[str]) -> bool:\n",
    "    p = normalize_eval_text(pred)\n",
    "    gs = [normalize_eval_text(g) for g in golds]\n",
    "    return any((p in g) or (g in p) for g in gs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAR3hmjv96-0"
   },
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(out, model_name, modality, question_type):\n",
    "\n",
    "    # Micro accuracy (overall)\n",
    "    micro_acc = float(out[\"correct\"].mean()) if len(out) else float(\"nan\")\n",
    "\n",
    "    # Per-category accuracy\n",
    "    by_cat = out.groupby([\"category\"])[\"correct\"].agg([\"mean\", \"count\"]).reset_index()\n",
    "    by_cat = by_cat.rename(columns={\"mean\": \"accuracy\", \"count\": \"n\"})\n",
    "\n",
    "    # Macro accuracy (mean over categories)\n",
    "    macro_acc = float(by_cat[\"accuracy\"].mean()) if len(by_cat) else float(\"nan\")\n",
    "\n",
    "    errors = out[out[\"correct\"] == 0].copy()\n",
    "\n",
    "    # Save\n",
    "    out.to_csv(SAVE_DIR / f\"predictions_{model_name}_{modality}_{question_type}.csv\", index=False)\n",
    "    errors.to_csv(SAVE_DIR / f\"errors_{model_name}_{modality}_{question_type}.csv\", index=False)\n",
    "    report = {\n",
    "        \"model\": model_name,\n",
    "        \"overall\": {\n",
    "            \"micro_accuracy\": micro_acc,\n",
    "            \"macro_accuracy\": macro_acc,\n",
    "            \"n_samples\": int(len(out))\n",
    "        },\n",
    "        \"by_category\": by_cat.to_dict(orient=\"records\"),\n",
    "        \"n_errors\": int(len(errors))\n",
    "    }\n",
    "    with open(SAVE_DIR / f\"report_{model_name}_{modality}_{question_type}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n=== {model_name} report ===\")\n",
    "    print(json.dumps(report, ensure_ascii=False, indent=2))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Pu5z_KLtdJ"
   },
   "source": [
    "# **Llama 3.1 (text + image)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4CVeOqavO36",
    "outputId": "143cc283-2f3d-448e-f0c6-71bef5c27b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]Still waiting to acquire lock on Meta-Llama-3.1-70B-Instruct/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Still waiting to acquire lock on Meta-Llama-3.1-70B-Instruct/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
      "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct/resolve/1605565b47bb9346c5515c34102e054115b4f98b/original/consolidated.00.pth\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/huggingface-cli\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/commands/huggingface_cli.py\", line 61, in main\n",
      "    service.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/commands/download.py\", line 157, in run\n",
      "    print(self._download())  # Print path to downloaded files\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/commands/download.py\", line 191, in _download\n",
      "    return snapshot_download(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/_snapshot_download.py\", line 332, in snapshot_download\n",
      "    thread_map(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\n",
      "    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\n",
      "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "               ^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 619, in result_iterator\n",
      "    yield _result_or_cancel(fs.pop())\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 317, in _result_or_cancel\n",
      "    return fut.result(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/_snapshot_download.py\", line 306, in _inner_hf_hub_download\n",
      "    return hf_hub_download(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 990, in hf_hub_download\n",
      "    return _hf_hub_download_to_local_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1253, in _hf_hub_download_to_local_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1658, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1546, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1463, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 426, in hf_raise_for_status\n",
      "    raise _format(GatedRepoError, message, response) from e\n",
      "huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-68c9a110-3eb0ec067ce0c59626594875;62c3839a-0806-41d1-b2ec-19ec18004dea)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct/resolve/1605565b47bb9346c5515c34102e054115b4f98b/original/consolidated.00.pth.\n",
      "Access to model meta-llama/Llama-3.1-70B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers accelerate torch --upgrade\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "!huggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-70B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3DFgyvZvQvC"
   },
   "source": [
    "# Llama 3.1\n",
    "text-only (Multiple Choice Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "XVj5kDgELMqR",
    "outputId": "327ccf70-c531-4c5c-e92c-dc463b54e7e4"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct.\n401 Client Error. (Request ID: Root=1-68c9a09b-659002186957fd323dc34745;d542aea0-59b1-433d-b4bd-45d7022de9e7)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-70B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    425\u001b[0m             )\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-68c9a09b-659002186957fd323dc34745;d542aea0-59b1-433d-b4bd-45d7022de9e7)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-70B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1169223860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-3.1-70B-Instruct\"\u001b[0m  \u001b[0;31m# or 8B / 405B if you have the hardware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m model = AutoModelForCausalLM.from_pretrained(\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1079\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    543\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct.\n401 Client Error. (Request ID: Root=1-68c9a09b-659002186957fd323dc34745;d542aea0-59b1-433d-b4bd-45d7022de9e7)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-70B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "invalid_answers = []\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-70B-Instruct\"  # or 8B / 405B if you have the hardware\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "with open(mcq_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "                    \"شما باید به سوال چند گزینه ای که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، لطفا با دقت سوال زیر را بخوانید و از بین گزینه های ارائه شده گزینه ی صحیح را انتخاب کنید\\n\\n \"\n",
    "                    \"سوال: {q}\\n\"\n",
    "                    \"گزینه ها:\\n\"\n",
    "                    \"1) {ctx[0]}\\n\"\n",
    "                    \"2) {ctx[1]}\\n\"\n",
    "                    \"3) {ctx[2]}\\n\"\n",
    "                    \"4) {ctx[3]}\\n\"\n",
    "                    \"لطفا فقط مقدار گزینه ی صحیح را بیان کن و هیچ توضیح اضافی نده\\n\"\n",
    "                ).format(q = record[\"question\"], ctx = record[\"options\"])\n",
    "\n",
    "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=1000)\n",
    "        pred = tok.decode(out[0], skip_special_tokens=True)\n",
    "        ############################################################################### Evaluation\n",
    "        if pred in record[\"options\"]:\n",
    "            ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "            rows.append({\n",
    "                \"category\": category,\n",
    "                \"question\": record[\"question\"],\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"prediction\": pred,\n",
    "                \"correct\": int(ok)\n",
    "            })\n",
    "        else:\n",
    "            invalid_answers.append(pred)\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    print(invalid_answers)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Llama_3_1\", modality = \"Text_only\", question_type = \"MCQ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_4EBuRgq2YQ"
   },
   "source": [
    "# Llama 3.1\n",
    "text-only (No answers, only questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FO4l9vhqri1"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-70B-Instruct\"  # or 8B / 405B if you have the hardware\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "with open(_110_unimodal_questions_without_answers_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "            \"شما باید به سوالی که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، لطفا با دقت سوال زیر را بخوانید و پاسخ صحیح را بدون توضیحات اضافی برگردانید\\n\\n \"\n",
    "            \"سوال: {q}\\n\"\n",
    "        ).format(q = record[\"question\"])\n",
    "\n",
    "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=1000)\n",
    "        pred = tok.decode(out[0], skip_special_tokens=True)\n",
    "        ############################################################################### Evaluation\n",
    "        ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "        rows.append({\n",
    "            \"category\": category,\n",
    "            \"question\": record[\"question\"],\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"prediction\": pred,\n",
    "            \"correct\": int(ok)\n",
    "        })\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Llama_3_1\", modality = \"Text_only\", question_type = \"No_answers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMo0zp2dNgSu"
   },
   "source": [
    "# Llama 3.2-Vision (multimodal)\n",
    "\n",
    "Model IDs (pick one):\n",
    "\n",
    "*   \"meta-llama/Llama-3.2-11B-Vision-Instruct\" (easier to run)\n",
    "*   \"meta-llama/Llama-3.2-90B-Vision-Instruct\" (heavier, best quality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p57ExCAkNfci"
   },
   "outputs": [],
   "source": [
    "# Min versions matter for mllama:\n",
    "# !pip install --upgrade \"transformers>=4.45.0\" torch pillow accelerate\n",
    "# (Optional) login if the repo is gated for you:\n",
    "huggingface-cli login\n",
    "\n",
    "import os, torch, requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from typing import Optional, Union\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAOx7ZnRO98J"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = os.getenv(\"LLAMA_VISION_MODEL\", \"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "\n",
    "# Load once (bf16 + device_map=\"auto\" will spread across available GPU(s))\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "def _load_image(img: Union[str, Image.Image]) -> Image.Image:\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img\n",
    "    if img.startswith(\"http://\") or img.startswith(\"https://\"):\n",
    "        return Image.open(BytesIO(requests.get(img, stream=True, timeout=30).content)).convert(\"RGB\")\n",
    "    return Image.open(img).convert(\"RGB\")\n",
    "\n",
    "def llama32_vision(\n",
    "    prompt: str,\n",
    "    image: Optional[Union[str, Image.Image]] = None,\n",
    "    max_new_tokens: int = 1000,\n",
    "    temperature: float = 0.0,  # deterministic baseline\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use text-only (image=None) or multimodal (image provided).\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        # Text-only chat\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(text=input_text, return_tensors=\"pt\").to(model.device)\n",
    "    else:\n",
    "        img = _load_image(image)\n",
    "        # Chat with an image turn; the processor handles special tokens\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(img, input_text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=(temperature > 0),\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    # Processor knows how to detokenize mllama chat output\n",
    "    return processor.decode(output[0])\n",
    "\n",
    "# --- Examples ---\n",
    "# 1) Text-only (multilingual control)\n",
    "# print(llama32_vision(\"Answer in Persian and English: What is RAG?\"))\n",
    "\n",
    "# 2) Text + Image (URL)\n",
    "# print(llama32_vision(\n",
    "#     \"Describe the chart and answer briefly in Spanish.\",\n",
    "#     image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/rabbit.jpg\"\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCnapP6HWakb"
   },
   "source": [
    "# Llama 3.2-Vision\n",
    "Multimodal (Multiple Choice Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZwXi2b3WFed"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "invalid_answers = []\n",
    "\n",
    "with open(mcq_multimodal_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "            \"شما باید به سوال چند گزینه ای که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، همراه با سوال لینک مربوط به یک تصویر نیز ارسال خواهد شد که سوال در رابطه با آن تصویر است. لطفا با دقت سوال زیر را بخوانید و از بین گزینه های ارائه شده گزینه ی صحیح را انتخاب کنید\\n\\n \"\n",
    "            \"سوال: {q}\\n\"\n",
    "            \"گزینه ها:\\n\"\n",
    "            \"1) {ctx[0]}\\n\"\n",
    "            \"2) {ctx[1]}\\n\"\n",
    "            \"3) {ctx[2]}\\n\"\n",
    "            \"4) {ctx[3]}\\n\"\n",
    "            \"لطفا فقط مقدار گزینه ی صحیح را بیان کن و هیچ توضیح اضافی نده\\n\"\n",
    "        ).format(q = record[\"question\"], ctx = record[\"options\"])\n",
    "\n",
    "        pred = llama32_vision(prompt = prompt, image = record[\"image_url\"])\n",
    "\n",
    "        ############################################################################### Evaluation\n",
    "        if pred in record[\"options\"]:\n",
    "            ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "            rows.append({\n",
    "                \"category\": category,\n",
    "                \"question\": record[\"question\"],\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"prediction\": pred,\n",
    "                \"correct\": int(ok)\n",
    "            })\n",
    "        else:\n",
    "            invalid_answers.append(pred)\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    print(invalid_answers)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Llama_3_2_VI\", modality = \"Multimodal\", question_type = \"MCQ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4DBtP3ZWqIq"
   },
   "source": [
    "# Llama 3.2-Vision\n",
    "Multimodal (No answers, only questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTTBcASIWGMR"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "with open(_50_multimodal_questions_without_answers_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "            \"شما باید به سوالی که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، همراه با سوال لینک مربوط به یک تصویر نیز ارسال خواهد شد که سوال در رابطه با آن تصویر است. لطفا با دقت سوال را بخوانید و با توجه به تصویر پاسخ صحیح را بدون توضیحات اضافی برگردانید \\n\\n \"\n",
    "            \"سوال: {q}\\n\"\n",
    "        ).format(q = record[\"question\"])\n",
    "\n",
    "        pred = llama32_vision(prompt = prompt, image = record[\"image_url\"])\n",
    "\n",
    "        ############################################################################### Evaluation\n",
    "        ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "        rows.append({\n",
    "            \"category\": category,\n",
    "            \"question\": record[\"question\"],\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"prediction\": pred,\n",
    "            \"correct\": int(ok)\n",
    "        })\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Llama_3_2_VI\", modality = \"Multimodal\", question_type = \"No_answers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WHA7G0jLq6y"
   },
   "source": [
    "# **Qwen 2.5-72B-Instruct (text + image)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keYynk8dwJuH"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLd7Z3Erv96_"
   },
   "source": [
    "# Qwen 2.5-72B-Instruct\n",
    "text-only (Multiple Choice Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYQsCnZAL8BE"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "invalid_answers = []\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "with open(mcq_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "                    \"شما باید به سوال چند گزینه ای که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، لطفا با دقت سوال زیر را بخوانید و از بین گزینه های ارائه شده گزینه ی صحیح را انتخاب کنید\\n\\n \"\n",
    "                    \"سوال: {q}\\n\"\n",
    "                    \"گزینه ها:\\n\"\n",
    "                    \"1) {ctx[0]}\\n\"\n",
    "                    \"2) {ctx[1]}\\n\"\n",
    "                    \"3) {ctx[2]}\\n\"\n",
    "                    \"4) {ctx[3]}\\n\"\n",
    "                    \"لطفا فقط مقدار گزینه ی صحیح را بیان کن و هیچ توضیح اضافی نده\\n\"\n",
    "                ).format(q = record[\"question\"], ctx = record[\"options\"])\n",
    "\n",
    "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=1000)\n",
    "        pred = tok.decode(out[0], skip_special_tokens=True)\n",
    "        ############################################################################### Evaluation\n",
    "        if pred in record[\"options\"]:\n",
    "            ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "            rows.append({\n",
    "                \"category\": category,\n",
    "                \"question\": record[\"question\"],\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"prediction\": pred,\n",
    "                \"correct\": int(ok)\n",
    "            })\n",
    "        else:\n",
    "            invalid_answers.append(pred)\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    print(invalid_answers)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Qwen_2_5\", modality = \"Text_only\", question_type = \"MCQ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhjmQFwOwOWS"
   },
   "source": [
    "# Qwen 2.5-72B-Instruct\n",
    "Text-only (No answers, only questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsWlC7R0wO7E"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "with open(_110_unimodal_questions_without_answers_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "            \"شما باید به سوالی که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، لطفا با دقت سوال زیر را بخوانید و پاسخ صحیح را بدون توضیحات اضافی برگردانید\\n\\n \"\n",
    "            \"سوال: {q}\\n\"\n",
    "        ).format(q = record[\"question\"])\n",
    "\n",
    "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=1000)\n",
    "        pred = tok.decode(out[0], skip_special_tokens=True)\n",
    "        ############################################################################### Evaluation\n",
    "        ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "        rows.append({\n",
    "            \"category\": category,\n",
    "            \"question\": record[\"question\"],\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"prediction\": pred,\n",
    "            \"correct\": int(ok)\n",
    "        })\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Qwen_2_5\", modality = \"Text_only\", question_type = \"No_answers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bwv6zQ7w17w"
   },
   "source": [
    "# **Qwen2.5-VL (Vision - Language)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ODzl7DJw9Fs"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate pillow torch --upgrade\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# !pip install pillow requests\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOnRO1otMCMu"
   },
   "source": [
    "# Qwen2.5-VL\n",
    "Multimodal (Multiple Choice Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZ1RJX06MEGD"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "invalid_answers = []\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # scale up if GPU budget available\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "\n",
    "with open(mcq_multimodal_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "            \"شما باید به سوال چند گزینه ای که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، همراه با سوال لینک مربوط به یک تصویر نیز ارسال خواهد شد که سوال در رابطه با آن تصویر است. لطفا با دقت سوال زیر را بخوانید و از بین گزینه های ارائه شده گزینه ی صحیح را انتخاب کنید\\n\\n \"\n",
    "            \"سوال: {q}\\n\"\n",
    "            \"گزینه ها:\\n\"\n",
    "            \"1) {ctx[0]}\\n\"\n",
    "            \"2) {ctx[1]}\\n\"\n",
    "            \"3) {ctx[2]}\\n\"\n",
    "            \"4) {ctx[3]}\\n\"\n",
    "            \"لطفا فقط مقدار گزینه ی صحیح را بیان کن و هیچ توضیح اضافی نده\\n\"\n",
    "        ).format(q = record[\"question\"], ctx = record[\"options\"])\n",
    "\n",
    "        # Download image\n",
    "        url = record[\"image_url\"]\n",
    "        img = Image.open(BytesIO(requests.get(url, timeout=20).content)).convert(\"RGB\")\n",
    "        img = im.resize((512, 512))\n",
    "        # im.save(\"resized.jpg\", quality=92)\n",
    "        # img = Image.open(\"example.jpg\")\n",
    "\n",
    "        inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=1000)\n",
    "        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        ############################################################################### Evaluation\n",
    "        if pred in record[\"options\"]:\n",
    "            ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "            rows.append({\n",
    "                \"category\": category,\n",
    "                \"question\": record[\"question\"],\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"prediction\": pred,\n",
    "                \"correct\": int(ok)\n",
    "            })\n",
    "        else:\n",
    "            invalid_answers.append(pred)\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    print(invalid_answers)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Qwen_2_5_VL\", modality = \"Multimodal\", question_type = \"MCQ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfg2OHaawmFA"
   },
   "source": [
    "# Qwen2.5-VL\n",
    "Multimodal (No answers, only questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5MVZOw8wlup"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # scale up if GPU budget available\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "\n",
    "with open(_50_multimodal_questions_without_answers_filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for record in data:\n",
    "        correct_answer = record[\"answer\"]\n",
    "        category = record[\"category\"]\n",
    "        prompt = (\n",
    "            \"شما باید به سوالی که برایتان ارسال می شود به زبان فارسی پاسخ بدهید، همراه با سوال لینک مربوط به یک تصویر نیز ارسال خواهد شد که سوال در رابطه با آن تصویر است. لطفا با دقت سوال را بخوانید و با توجه به تصویر پاسخ صحیح را بدون توضیحات اضافی برگردانید \\n\\n \"\n",
    "            \"سوال: {q}\\n\"\n",
    "        ).format(q = record[\"question\"])\n",
    "\n",
    "        # Download image\n",
    "        url = record[\"image_url\"]\n",
    "        img = Image.open(BytesIO(requests.get(url, timeout=20).content)).convert(\"RGB\")\n",
    "        img = im.resize((512, 512))\n",
    "        # im.save(\"resized.jpg\", quality=92)\n",
    "        # img = Image.open(\"example.jpg\")\n",
    "\n",
    "        inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=1000)\n",
    "        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        ############################################################################### Evaluation\n",
    "        ok = exact_match(pred, [correct_answer]) if MATCH_MODE == \"exact\" else relaxed_match(pred, correct_answer)\n",
    "\n",
    "        rows.append({\n",
    "            \"category\": category,\n",
    "            \"question\": record[\"question\"],\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"prediction\": pred,\n",
    "            \"correct\": int(ok)\n",
    "        })\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    display(results)\n",
    "    evaluate_and_save_results(out = results, model_name = \"Qwen_2_5_VL\", modality = \"Multimodal\", question_type = \"No_answers\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
